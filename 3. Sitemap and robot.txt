Run this command in your terminal:

bash
Copy code
npm install next-sitemap
or with Yarn:

bash
Copy code
yarn add next-sitemap




-------------------------------------------------------------------------------------------------------------------
In the root directory of your project, create a file named next-sitemap.config.js
-------------------------------------------------------------------------------------------------------------------
const priorityMap = {
  '/': 1.0,             // Homepage has the highest priority
  '/special-page': 0.9, // Special page with high priority
  '/about': 0.8,        // About page with medium-high priority
  '/contact': 0.7,      // Contact page with medium priority
  '/blog': 0.6,         // Blog page with lower priority
  '/terms': 0.5,        // Terms page with even lower priority
};

module.exports = {
  siteUrl: process.env.NEXT_PUBLIC_BASE_URL || 'https://www.bdword.com',
  generateRobotsTxt: true,
  sitemapSize: 5000,
  changefreq: 'daily',
  robotsTxtOptions: {
    policies: [
    // âœ… Allow good bots----------------------------------------------------------------
      { userAgent: 'Googlebot', allow: '/' },
      { userAgent: 'Bingbot', allow: '/' },
      { userAgent: 'DuckDuckBot', allow: '/' },
      { userAgent: 'Slurp', allow: '/' },

    // ðŸš« Block known bad bots-----------------------------------------------------------
      { userAgent: 'AhrefsBot', disallow: '/' },      // SEO Crawler
      { userAgent: 'SemrushBot', disallow: '/' },     // SEO Tool
      { userAgent: 'MJ12bot', disallow: '/' },        // Link Analysis Bot
      { userAgent: 'DotBot', disallow: '/' },         // Scraper
      { userAgent: 'Baiduspider', disallow: '/' },    // Baidu (China)
      { userAgent: 'YandexBot', disallow: '/' },      // Yandex (Russia)
      { userAgent: 'Sogou', disallow: '/' },          // Chinese Bot
      { userAgent: 'Exabot', disallow: '/' },         // French Bot
      { userAgent: 'PetalBot', disallow: '/' },       // Huawei Bot
      { userAgent: 'crawler', disallow: '/' },        // Generic crawlers
      { userAgent: 'python-requests', disallow: '/' },// Python scripts
      { userAgent: 'wget', disallow: '/' },           // Wget tool
      { userAgent: 'curl', disallow: '/' },           // Curl tool

    // ðŸ”’ Disallow sensitive/private areas-------------------------------------------------
      { userAgent: '*', disallow: '/admin' },
      { userAgent: '*', disallow: '/api/private' },
      { userAgent: '*', disallow: '/server' },
      { userAgent: '*', disallow: '/config' },
      { userAgent: '*', disallow: '/.env' },
      { userAgent: '*', disallow: '/private' },
      { userAgent: '*', disallow: '/*.pdf' },
      { userAgent: '*', disallow: '/*.json' },
      { userAgent: '*', disallow: '/scripts' },
    ],
    additionalSitemaps: [
      `${process.env.NEXT_PUBLIC_BASE_URL || 'https://www.bdword.com'}/sitemap.xml`,
    ],
  },
  transform: async (config, url) => {
    // Set priority based on the URL from the priorityMap
    const priority = priorityMap[url] || (url === '/' ? 1.0 : 0.7);
    
    return {
      loc: url,
      priority: priority,
      changefreq: 'daily',
    };
  },
};

-------------------------------------------------------------------------------------------------------------------
In the root directory of your project, create a file named next-sitemap.config.js
-------------------------------------------------------------------------------------------------------------------


package.json
"scripts": {
  "dev": "next dev",
  "build": "next build",
  "start": "next start",
  "postbuild": "next-sitemap"  // Add this line
}
















---------------------------------------------------------------------------Home page = 1.0 and rest pages to 0.9-----------------------------------------------------------------------------------
module.exports = {
  siteUrl: process.env.NEXT_PUBLIC_BASE_URL || 'https://www.bdword.com',
  generateRobotsTxt: true,
  sitemapSize: 5000,
  changefreq: 'daily',
  robotsTxtOptions: {
    policies: [
      // âœ… Allow good bots----------------------------------------------------------------
      { userAgent: 'Googlebot', allow: '/' },
      { userAgent: 'Bingbot', allow: '/' },
      { userAgent: 'DuckDuckBot', allow: '/' },
      { userAgent: 'Slurp', allow: '/' },

      // ðŸš« Block known bad bots-----------------------------------------------------------
      { userAgent: 'AhrefsBot', disallow: '/' },      // SEO Crawler
      { userAgent: 'SemrushBot', disallow: '/' },     // SEO Tool
      { userAgent: 'MJ12bot', disallow: '/' },        // Link Analysis Bot
      { userAgent: 'DotBot', disallow: '/' },         // Scraper
      { userAgent: 'Baiduspider', disallow: '/' },    // Baidu (China)
      { userAgent: 'YandexBot', disallow: '/' },      // Yandex (Russia)
      { userAgent: 'Sogou', disallow: '/' },          // Chinese Bot
      { userAgent: 'Exabot', disallow: '/' },         // French Bot
      { userAgent: 'PetalBot', disallow: '/' },       // Huawei Bot
      { userAgent: 'crawler', disallow: '/' },        // Generic crawlers
      { userAgent: 'python-requests', disallow: '/' },// Python scripts
      { userAgent: 'wget', disallow: '/' },           // Wget tool
      { userAgent: 'curl', disallow: '/' },           // Curl tool

      // ðŸ”’ Disallow sensitive/private areas-------------------------------------------------
      { userAgent: '*', disallow: '/admin' },
      { userAgent: '*', disallow: '/api/private' },
      { userAgent: '*', disallow: '/server' },
      { userAgent: '*', disallow: '/config' },
      { userAgent: '*', disallow: '/.env' },
      { userAgent: '*', disallow: '/private' },
      { userAgent: '*', disallow: '/*.pdf' },
      { userAgent: '*', disallow: '/*.json' },
      { userAgent: '*', disallow: '/scripts' },
    ],
    additionalSitemaps: [
      `${process.env.NEXT_PUBLIC_BASE_URL || 'https://www.bdword.com'}/sitemap.xml`,
    ],
  },
  transform: async (config, url) => {
    // Set homepage to 1.0 priority, other pages to 0.9 priority
    const priority = url === '/' ? 1.0 : 0.9;
    
    return {
      loc: url,
      priority: priority,
      changefreq: 'daily',
    };
  },
};










---------------------------------------------------------------------------With dynamic files from json-----------------------------------------------------------------------------------

Summary:--------------------------------------------------
Install next-sitemap.
Create and configure next-sitemap.js.
Update next.config.js to include next-sitemap.
Add words.json and adjust to your project.
Build your project to generate sitemap.xml and robots.txt.
Deploy your project.--------------------------------------



const fs = require('fs');

// Load words from a JSON file to avoid runtime DB load
const words = JSON.parse(fs.readFileSync('words.json', 'utf-8'));

// Define the languages you support
const languages = ['bengali', 'hindi', 'urdu', 'french']; // Add more as needed

// Generate dynamic paths for each word-language combination
const generateDynamicPaths = () => {
  return words.flatMap((word) =>
    languages.map((language) => ({
      loc: `https://www.english-welsh.net/english-to-${language}-meaning-${word}`,
      lastmod: new Date().toISOString(),
      changefreq: 'daily',
      priority: 0.7,
    }))
  );
};

module.exports = {
  siteUrl: process.env.NEXT_PUBLIC_BASE_URL_FRONT || 'https://www.english-welsh.net',
  generateRobotsTxt: true,
  sitemapSize: 5000,
  changefreq: 'daily',
  outDir: './public',  // Output the sitemap to the public folder
  robotsTxtOptions: {
    policies: [
      // âœ… Allow good bots
      { userAgent: 'Googlebot', allow: '/' },
      { userAgent: 'Bingbot', allow: '/' },
      { userAgent: 'DuckDuckBot', allow: '/' },
      { userAgent: 'Slurp', allow: '/' },

      // ðŸš« Block known bad bots
      { userAgent: 'AhrefsBot', disallow: '/' },      // SEO Crawler
      { userAgent: 'SemrushBot', disallow: '/' },     // SEO Tool
      { userAgent: 'MJ12bot', disallow: '/' },        // Link Analysis Bot
      { userAgent: 'DotBot', disallow: '/' },         // Scraper
      { userAgent: 'Baiduspider', disallow: '/' },    // Baidu (China)
      { userAgent: 'YandexBot', disallow: '/' },      // Yandex (Russia)
      { userAgent: 'Sogou', disallow: '/' },          // Chinese Bot
      { userAgent: 'Exabot', disallow: '/' },         // French Bot
      { userAgent: 'PetalBot', disallow: '/' },       // Huawei Bot
      { userAgent: 'crawler', disallow: '/' },        // Generic crawlers
      { userAgent: 'python-requests', disallow: '/' },// Python scripts
      { userAgent: 'wget', disallow: '/' },           // Wget tool
      { userAgent: 'curl', disallow: '/' },           // Curl tool

      // ðŸ”’ Disallow sensitive/private areas
      { userAgent: '*', disallow: '/admin' },
      { userAgent: '*', disallow: '/api/private' },
      { userAgent: '*', disallow: '/server' },
      { userAgent: '*', disallow: '/config' },
      { userAgent: '*', disallow: '/.env' },
      { userAgent: '*', disallow: '/private' },
      { userAgent: '*', disallow: '/*.pdf' },
      { userAgent: '*', disallow: '/*.json' },
      { userAgent: '*', disallow: '/scripts' },
    ],
    additionalSitemaps: [
      `${process.env.NEXT_PUBLIC_BASE_URL_FRONT || 'https://www.english-welsh.net'}/sitemap.xml`,
    ],
  },
  transform: async (config, url) => {
    // Static pages you want to include in the sitemap
    const staticPages = [
      '/',                // Home page
      '/about',           // About page
      '/contact',         // Contact page
      '/terms',           // Terms page
      '/privacy-policy',  // Privacy Policy page
      // Add more static routes here as needed
    ];

    // If the URL is a static page, assign a higher priority (1.0), otherwise 0.7
    const priority = staticPages.includes(url) ? 1.0 : 0.7;

    return {
      loc: url,
      priority: priority,
      changefreq: 'daily', // Set the frequency of changes
    };
  },
  additionalPaths: async (config) => {
    return generateDynamicPaths(); // Generate dynamic pages based on the words from the JSON file
  },
};


